# Necessary for accessing configuration
# from symmstate.config import settings

# def run_calculation():
#     print(f"Using pseudopotentials from: {settings.PP_DIR}")
#     print(f"Environment: {settings.ENVIRONMENT}")
#     print(f"Default ecut: {settings.DEFAULT_ECUT}")

# Overriding settings: 
# export SYMMSTATE_PP_DIR="/new/path/to/pseudos"
# export SYMMSTATE_ENVIRONMENT="development"

# YAML config file: 
# from symmstate.config import Settings

# custom_settings = Settings(_env_file="custom_config.yaml")



########## DESIGN PATTERNS

# 1.) Need different strategy classes perhaps? 

# 
# from abc import ABC, abstractmethod

# class ConvergenceStrategy(ABC):
#     @abstractmethod
#     def is_converged(self, results: dict) -> bool:
#         pass

# class EnergyConvergence(ConvergenceStrategy):
#     def __init__(self, tolerance=1e-6):
#         self.tolerance = tolerance

#     def is_converged(self, results):
#         return abs(results['energy_diff']) < self.tolerance

# class ForceConvergence(ConvergenceStrategy):
#     def __init__(self, max_force=0.01):
#         self.max_force = max_force

#     def is_converged(self, results):
#         return all(f < self.max_force for f in results['forces'])

# # Usage
# convergence_strategy = EnergyConvergence(tolerance=1e-8)
# simulation.set_convergence_strategy(convergence_strategy)


# Complex object  creation for different unit cell types: 

# class UnitCellFactory:
#     @classmethod
#     def from_abinit_file(cls, file_path: str) -> UnitCell:
#         # Parse file and validate
#         params = AbinitParser.parse(file_path)
#         return UnitCell(**params)

#     @classmethod
#     def from_smodes(cls, smodes_file: str, irrep: str) -> UnitCell:
#         # Generate symmetry-adapted basis
#         basis = SymmetryAdapter.generate_basis(smodes_file, irrep)
#         return UnitCell(**basis)

#     @classmethod
#     def from_direct_input(cls, acell, rprim, coords) -> UnitCell:
#         # Validate direct inputs
#         return UnitCell(acell, rprim, coords)

# # Usage
# cell = UnitCellFactory.from_abinit_file("structure.abi")



###### For monitoring long running simulations: 

# class SimulationSubject:
#     def __init__(self):
#         self._observers = []

#     def attach(self, observer):
#         self._observers.append(observer)

#     def notify(self, event_type, data):
#         for observer in self._observers:
#             observer.update(event_type, data)

# class ProgressObserver:
#     def update(self, event_type, data):
#         if event_type == "ITERATION":
#             print(f"Iteration {data['step']}: Energy {data['energy']}")
#         elif event_type == "CONVERGED":
#             print("Simulation converged!")

# # Usage
# simulation = AbinitSimulation()
# simulation.attach(ProgressObserver())
# simulation.attach(FileLoggerObserver())


###### FOr adding logging/validation to core operators

# class CalculationDecorator:
#     def __init__(self, calculator):
#         self.calculator = calculator

#     def run(self):
#         self.pre_process()
#         result = self.calculator.run()
#         self.post_process()
#         return result

# class LoggingDecorator(CalculationDecorator):
#     def pre_process(self):
#         print(f"Starting {type(self.calculator).__name__} at {datetime.now()}")

#     def post_process(self):
#         print(f"Completed in {time_elapsed} seconds")

# class ValidationDecorator(CalculationDecorator):
#     def pre_process(self):
#         if not self.calculator.inputs_valid():
#             raise InvalidInputError

# # Usage
# calculator = LoggingDecorator(
#     ValidationDecorator(
#         EnergyCalculator()
#     )
# )
# result = calculator.run()


##### Template Method Pattern 
#### Common workflow for different calculations

# class SimulationTemplate(ABC):
#     def run(self):
#         self.initialize()
#         self.setup_parameters()
#         self.execute()
#         self.post_process()

#     @abstractmethod
#     def initialize(self): pass
    
#     @abstractmethod
#     def setup_parameters(self): pass

#     def execute(self):
#         # Default execution method
#         self.solver.run()

#     def post_process(self):
#         # Default post-processing
#         self.save_results()

# class FlexoelectricSimulation(SimulationTemplate):
#     def setup_parameters(self):
#         self.solver.set_strain(0.01)
#         self.solver.set_electric_field(0.1)

# class PhononSimulation(SimulationTemplate):
#     def setup_parameters(self):
#         self.solver.set_qpoints([0,0,0])


#### Managing simulation results. Abstract data access layer: 


# class ResultRepository:
#     def __init__(self, storage: ResultStorage):
#         self.storage = storage  # Could be SQL, HDF5, JSON

#     def save(self, result: SimulationResult):
#         self.storage.persist(result)

#     def get_by_criteria(self, criteria: dict):
#         return self.storage.query(criteria)

# class HDF5Storage:
#     def persist(self, result):
#         with h5py.File("results.h5", "a") as f:
#             # Store in HDF5 format

# class SQLStorage:
#     def persist(self, result):
#         # Store in relational database

#Start with Factory Pattern: Simplify object creation

# Add Strategy Pattern: For convergence criteria

# Implement Observer: For monitoring simulations

# Introduce Decorators: For cross-cutting concerns

# Build Template Methods: Standardize workflows



#### FOR EXISTING WORKFLOWS

# 1.) Architectural improvements: 

# Strategy Pattern for Calculations: 

# class CalculationStrategy(ABC):
#     @abstractmethod
#     def calculate(self, perturbation_object):
#         pass

# class EnergyStrategy(CalculationStrategy):
#     def calculate(self, perturbation_object):
#         perturbation_object.run_energy_calculation()

# class PiezoStrategy(CalculationStrategy):
#     def calculate(self, perturbation_object):
#         perturbation_object.run_piezo_calculation()

# # Usage in Perturbations
# def run_calculations(self, strategy: CalculationStrategy):
#     for obj in self.perturbed_objects:
#         strategy.calculate(obj)

# 1.2) Factor Pattern for File Creation 

# class AbinitFileFactory:
#     @staticmethod
#     def create_energy_file(base_file):
#         return AbinitFile(base_file, calc_type='energy')

#     @staticmethod
#     def create_flexo_file(base_file):
#         return AbinitFile(base_file, calc_type='flexo')


# 2.) Code Quality Enhancements

#2.1 Centralized Configuration: 

# # config.py
# from pydantic import BaseSettings

# class Settings(BaseSettings):
#     SMODES_PATH: str = "../isobyu/smodes"
#     DEFAULT_HOST_SPEC: str = "mpirun -np 30"
#     SYMM_PREC: float = 1e-5

# settings = Settings()


# 2.2) Improved Error Handling

# In SmodesProcessor.run_smodes()
# try:
#     result = subprocess.run(command, check=True, capture_output=True, text=True)
# except subprocess.CalledProcessError as e:
#     self.logger.error(f"SMODES failed: {e.stderr}")
#     raise RuntimeError("SMODES execution failed") from e


# 3.) Performance Optimizations

# 3.1 Vectorized Operations 
# # Before
# for i in range(len(selected_indices)):
#     ax.plot(cleaned_amps, plot_data[:, i], ...)

# # After (using matplotlib's object-oriented interface)
# ax.plot(cleaned_amps, plot_data, '-o', label=labels)

# 3.2 Memory Management: 

# # Use context managers for file operations
# with h5py.File("results.h5", "w") as hf:
#     hf.create_dataset("phonon_vecs", data=self.phonon_vecs)

# 4.) Testing Infrastructure

# 4.1 Pytest Fixtures 

# # Use context managers for file operations
# with h5py.File("results.h5", "w") as hf:
#     hf.create_dataset("phonon_vecs", data=self.phonon_vecs)


# 5.) Documentation & Maintainability

# 5.1) Type Hints and Docstrings: 

# def calculate_energy_of_perturbations(self) -> None:
#     """Run energy calculations for all perturbed configurations.
    
#     Raises:
#         RuntimeError: If job submission fails
#     """


# 5.2 Logging Configuration:

# # utils/logger.py
# import logging

# def get_logger(name):
#     logger = logging.getLogger(name)
#     logger.setLevel(logging.INFO)
#     # Add handlers
#     return logger


# 6.) Workflow-Specific Improvements 

# 6.1 For Perturbations Class
# Refractor Calculation Methods: 
# def _run_calculation_flow(self, calculation_type):
#     for obj in self.perturbed_objects:
#         obj.run_calculation(calculation_type)
#         self._handle_post_calculation(obj)

# 6.2 For SmodesProcessor 
# def _stabilize_force_matrix(self):
#     self.force_matrix = self.stabilize_matrix(
#         self.force_matrix, 
#         threshold=1e5
#     )

# Critical Security/Stability Fixes: 

# 1.) Subprocess Sanitization: 
# In SmodesProcessor.run_smodes()
# sanitized_input = shlex.quote(smodes_input)
# command = f"{self.smodes_path} < {sanitized_input}"

# Numerical Stability: 

# # In _perform_calculations()
# self.dyn_mat = np.divide(fc_mat, mass_matrix, where=mass_matrix != 0)


# IF I DO THIS, this is some benefits: 

#BEFORE: 
# # In Perturbations
# def calculate_energy_of_perturbations(self):
#     for obj in self.perturbed_objects:
#         obj.run_energy_calculation()
#         # ... manual job tracking ...


#AFTER: 
# def calculate_energy_of_perturbations(self):
#     strategy = EnergyCalculationStrategy()
#     self.run_calculations(strategy)
#     self._process_energy_results()



###### ENHANCED SLURMFILE CLASS

# slurm_file.py
# class SlurmFile(SymmStateCore):
#     def __init__(self, sbatch_header_source):
#         super().__init__()
#         self.job_graph = {}  # {job_name: (job_id, [dependencies])}
    
#     def submit_job(self, job_name, script_path, dependencies=None):
#         """Submit job with dependencies and track relationships"""
#         dependency_flags = []
#         if dependencies:
#             dep_ids = [self.job_graph[dep][0] for dep in dependencies]
#             dependency_flags = [f"--dependency=afterok:{':'.join(dep_ids)}"]

#         result = subprocess.run(
#             ["sbatch", *dependency_flags, script_path],
#             capture_output=True,
#             text=True
#         )
        
#         if result.returncode == 0:
#             job_id = result.stdout.split()[-1]
#             self.job_graph[job_name] = (job_id, dependencies or [])
#             return job_id
#         else:
#             raise JobSubmissionError(f"Failed to submit {job_name}: {result.stderr}")




########### WORKFLOW BUILDER CLASS


# workflow_manager.py
# from collections import defaultdict

# class WorkflowManager:
#     def __init__(self, slurm_manager):
#         self.slurm = slurm_manager
#         self.dependencies = defaultdict(list)
        
#     def add_step(self, name, script_path, depends_on=None):
#         """Add workflow step with dependencies"""
#         job_id = self.slurm.submit_job(name, script_path, depends_on)
#         if depends_on:
#             for dep in depends_on:
#                 self.dependencies[dep].append(name)
#         return job_id

#     def visualize_workflow(self):
#         """Generate ASCII workflow diagram"""
#         print("Workflow Graph:")
#         for job, (_, deps) in self.slurm.job_graph.items():
#             print(f"{job} [{len(deps)} dependencies]")
#             for dep in deps:
#                 print(f"  └─ depends on {dep}")



######## EXAMPLE WITH ABINIT WORKFLOW

# def run_flexo_workflow():
#     slurm = SlurmFile("slurm_header.sh")
#     workflow = WorkflowManager(slurm)
    
#     # Initial structure optimization
#     optimization_job = workflow.add_step(
#         "optimization", 
#         "optimize_structure.sh"
#     )
    
#     # Parallel phonon calculations
#     phonon_jobs = []
#     for i in range(8):
#         job_id = workflow.add_step(
#             f"phonon_{i}",
#             f"phonon_{i}.sh",
#             depends_on=["optimization"]
#         )
#         phonon_jobs.append(job_id)
    
#     # Final analysis (depends on all phonon jobs)
#     analysis_job = workflow.add_step(
#         "analysis",
#         "process_results.sh",
#         depends_on=phonon_jobs
#     )
    
#     workflow.visualize_workflow()
#     return slurm.job_graph



##### ADVANCED FEATURES
# Retries
# def submit_with_retry(self, job_name, script_path, max_retries=3):
#     for attempt in range(max_retries):
#         try:
#             return self.submit_job(job_name, script_path)
#         except JobSubmissionError:
#             if attempt == max_retries - 1:
#                 raise
#             time.sleep(60 * 2**attempt)  # Exponential backoff


# Resource Grouping

# def create_job_array(self, base_script, num_jobs):
#     """Create SLURM job array for parameter sweeps"""
#     with open("array_wrapper.sh", "w") as f:
#         f.write(f"""#!/bin/bash
# #SBATCH --array=1-{num_jobs}
# {base_script} $SLURM_ARRAY_TASK_ID
# """)
#     return self.submit_job(f"array_{base_script}", "array_wrapper.sh")


#### BEST PRACTICE WITH TEMPLATES: 

# def generate_abinit_script(template, params):
#     """Use Jinja2 for dynamic script generation"""
#     from jinja2 import Template
#     return Template(template).render(params)

### BEST WITH MONITORING WORKFLOWS

# def monitor_workflow(job_graph):
#     while True:
#         running = 0
#         for name, (job_id, _) in job_graph.items():
#             status = subprocess.run(
#                 ["squeue", "-j", job_id], 
#                 capture_output=True
#             )
#             if "RUNNING" in status.stdout:
#                 running += 1
#         print(f"Jobs remaining: {running}")
#         if running == 0:
#             break
#         time.sleep(300)  # Check every 5 minutes



##### ERROR HANDLING

# class WorkflowFailed(Exception):
#     pass

# def validate_output(job_graph):
#     for name, (job_id, _) in job_graph.items():
#         result = subprocess.run(
#             ["sacct", "-j", job_id, "--format=State"],
#             capture_output=True,
#             text=True
#         )
#         if "FAILED" in result.stdout:
#             raise WorkflowFailed(f"Job {name} ({job_id}) failed")


# BENEFITS

# Explicit Dependencies - Ensures proper execution order

# Visual Tracking - See relationships between jobs

# Error Propagation - Automatic failure detection

# Resource Efficiency - Maximize cluster usage

# Reusability - Works with existing AbinitFile/SlurmFile


### EXCLUSION FLOW: 

# graph TD
#     A[Structure Optimization] --> B[Phonon Calculation 1]
#     A --> C[Phonon Calculation 2]
#     A --> D[Phonon Calculation 3]
#     B --> E[Analysis]
#     C --> E
#     D --> E